1-1 For which reason is it better to run the container with a flag -e to give the environment variables rather than put them directly in the Dockerfile?

It is better to use the -e flag to set environment variables when running a container because it allows for greater flexibility and security. 
By using the -e flag, you can easily change the environment variables without needing to modify the Dockerfile and rebuild the image. 
This is particularly useful for sensitive information like database credentials, as it prevents hardcoding them into the Dockerfile, reducing the risk of exposing them in version control systems. 
Additionally, it allows for different configurations in different environments (development, testing, production) without changing the Docker image itself.


1-2 Why do we need a volume to be attached to our postgres container?

A volume is needed to be attached to a Postgres container to ensure data persistence.
Without a volume, any data stored in the container's filesystem would be lost when the container is stopped or removed. 
By using a volume, the data is stored outside the container's lifecycle, allowing it to persist even if the container is recreated. 
This is crucial for databases like Postgres, where data integrity and availability are paramount.


1-3 Document your database container essentials: commands and Dockerfile.

Dockerfile:

FROM postgres:17.2-alpine
ENV POSTGRES_DB=db \
   POSTGRES_USER=usr \
   POSTGRES_PASSWORD=pwd
COPY CreateScheme.sql /docker-entrypoint-initdb.d/01-create-scheme.sql
COPY InsertData.sql /docker-entrypoint-initdb.d/02-insert-data.sql

Commands to build and run the container:

docker build -t mon-postgres -f TP1/Database/Dockerfile .
docker run -d --name postgres-db --network app-network -p 5432:5432 mon-postgres


1-4 Why do we need a multistage build? And explain each step of this dockerfile.

Multistage builds allow us to reduce the size of the final Docker image by separating the build environment from the runtime environment.
The security and performance of the final image are improved because it only contains the necessary components to run the application, without any build tools or dependencies.

Step 1: Build
FROM eclipse-temurin:21-jdk-alpine AS myapp-build

Use the Eclipse Temurin JDK 21 Alpine image as the base for building the application, and name this stage "myapp-build".

ENV MYAPP_HOME=/opt/myapp
Set an environment variable MYAPP_HOME to specify the application home directory. Facilitates easier management of file paths.

WORKDIR $MYAPP_HOME
Set the working directory to MYAPP_HOME for subsequent commands.

RUN apk add --no-cache maven
Install maven in the build environment to handle Java project dependencies and build processes.

COPY pom.xml .
Copy the Maven project file (pom.xml) into the working directory.

COPY src ./src
Copy the source code of the application into the working directory.

RUN mvn package -DskipTests
Build the application using Maven, packaging it into a JAR file while skipping tests to speed up the build process.



Step 2: Runtime
FROM eclipse-temurin:21-jre-alpine
Use the Eclipse Temurin JRE 21 Alpine image as the base for the runtime environment, which is lighter than the JDK image.

ENV MYAPP_HOME=/opt/myapp
WORKDIR $MYAPP_HOME
Set the same environment variable and working directory for consistency.

COPY --from=myapp-build $MYAPP_HOME/target/*.jar $MYAPP_HOME/myapp.jar
Copy the built JAR file from the "myapp-build" stage into the runtime image.

ENTRYPOINT ["java", "-jar", "myapp.jar"]
Define the command to run the application when the container starts, executing the JAR file with Java.


1-5 Why do we need a reverse proxy?

A reverse proxy is needed for several reasons:

Load Balancing: It can distribute incoming traffic across multiple backend servers, improving performance and reliability.

SSL Termination: It can handle SSL encryption and decryption, offloading this resource-intensive process from the backend servers.

Caching: It can cache responses from the backend servers, reducing the load on those servers and improving response times for clients.

Security: It can act as an additional layer of security, hiding the details of the backend servers and protecting them from direct exposure to the internet.

Compression: It can compress responses before sending them to clients, reducing bandwidth usage and improving load times.


1-6 Why is docker-compose so important?

Docker-compose is important because it simplifies the process of managing multi-container Docker applications.
With docker-compose, you can define and configure all the services, networks, and volumes needed for your application in a single YAML file.


1-7 Document docker-compose most important commands.

docker-compose up -d
Starts and runs the containers defined in the docker-compose.yml file in detached mode.

docker-compose down
Stops and removes the containers, networks, and volumes defined in the docker-compose.yml file.

docker-compose logs -f [service_name]
Displays the logs of all services or a specific service in real-time.

docker-compose build [service_name]
Builds or rebuilds the services defined in the docker-compose.yml file.

docker-compose ps
Lists the status of the containers defined in the docker-compose.yml file.


1-8 Document your docker-compose file.

1. SERVICE DEPENDENCIES (depends_on)
Services are started in order based on their dependencies:
database → backend → httpd
The 'condition: service_healthy' ensures that a service only starts when its
dependency is fully operational and passes its health check.

2. HEALTH CHECKS
Each service has a health check that Docker uses to determine if the service
is ready to accept connections:
- database: Uses pg_isready to verify PostgreSQL is accepting connections
- backend: Uses wget to check if the API endpoint responds
- httpd: No explicit health check (starts immediately after backend is healthy)
Health check parameters:
- interval: How often to run the check
- timeout: Maximum time to wait for a response
- retries: Number of consecutive failures before marking as unhealthy
- start_period: Grace period before health checks start (for slow-starting apps)

3. NETWORKS
The 'app-network' is a custom bridge network that allows containers to
communicate with each other using their service names as hostnames.
Example: The backend connects to 'postgres-db:5432' instead of an IP address.

4. VOLUMES
The 'db-data' volume provides persistent storage for PostgreSQL data.
This ensures that database data survives container restarts and removals.
Without a volume, all data would be lost when the container is removed.

5. ENVIRONMENT VARIABLES
Environment variables are loaded from the .env file in the same directory.
This allows sensitive information (passwords, usernames) to be kept separate
from the docker-compose.yml file and not committed to version control.

6. BUILD CONTEXT
Each service specifies a build context pointing to the directory containing
its Dockerfile. Docker Compose will automatically build images if they don't
exist or if you use the --build flag.

7. RESTART POLICY
'restart: unless-stopped' ensures containers automatically restart if they
crash or if the Docker daemon restarts, unless explicitly stopped by the user.
This improves application resilience in production environments.

8. PORT MAPPING
Only the httpd service exposes a port to the host machine (80:80).
The database and backend are only accessible within the Docker network,
following the principle of least exposure for security.




1-10 Why do we put our images into an online repo?

We put our images into an online repository (like Docker Hub or a private registry) for several reasons.
It allows easy sharing and distribution of images across different environments and teams, facilitates collaboration, and streamlines the CI/CD process.

2-1 What are testcontainers?

Testcontainers is a Java library that provides lightweight, throwaway instances of common databases, Selenium web browsers, or anything else that can run in a Docker container.
They are primarily used for integration testing, allowing developers to run tests against real services in isolated environments.


2-2 For what purpose do we need to use secured variables ?

Secured variables are used to protect sensitive information such as passwords, API keys, and tokens from being exposed in code repositories or logs.


2-3 Why did we put needs: build-and-test-backend on this job? Maybe try without this and you will see!

The "needs: build-and-test-backend" directive in a CI/CD pipeline job specifies that the current job depends on the successful completion of the "build-and-test-backend" job.
This ensures that the current job will only run if the backend build and tests have passed, maintaining the integrity of the deployment process.


2-4 For what purpose do we need to push docker images?

Pushing Docker images to a registry allows for easy distribution and deployment of applications across different environments.
It enables teams to share images, facilitates version control, and supports CI/CD workflows by providing a centralized location for storing and retrieving images.



3-1 Document your inventory and base commands

ansible all -i inventories/setup.yml -m ping

This command checks the connectivity to all hosts defined in the inventory file.

ansible all -i inventories/setup.yml -m setup -a "filter=ansible_distribution*"

This command gathers and displays facts about the operating system distribution of all hosts defined in the inventory file.

ansible all -m apt -a "name=apache2 state=absent" --become 

This command removes Apache2 from all hosts defined in the inventory file, using elevated privileges.


3-2 Document your playbook

The playbook executes 5 roles sequentially: docker, network, database, app, proxy. Each role isolates specific deployment concerns.
We have for example [docker, network, database, app, proxy]
It uses variables defined in group_vars/all.yml for sensitive information like database credentials and network names.


3-3 Document your docker_container tasks configuration.

The docker_container tasks ensure that each service is properly configured and connected to the appropriate networks. For example, the PostgreSQL container uses a healthcheck to ensure it is ready before dependent services start. It is connected only to the backend-network for isolation.
The backend API is connected to both networks (backend + frontend) acting as a bridge. Uses SPRING_DATASOURCE_URL to connect to postgres-db via Docker DNS.
The proxy service exposes port 80 and connects only to frontend-network, preventing direct database access. Uses restart_policy: always for high availability.
We also have Two isolated networks: backend-network (DB↔API only) and frontend-network (API↔Proxy only). This prevents the proxy from accessing the database directly.


3-4 : Is it really safe to deploy automatically every new image on the hub ? explain. What can I do to make it more secure?

Automatically deploying every new image to a public or shared Docker Hub repository can pose security risks, especially if the images contain sensitive information or vulnerabilities. If an image is compromised, it could lead to unauthorized access or exploitation of systems that use that image.
To make the deployment process more secure, we need to implement measures like image scanning for vulnerabilities before deployment, using private repositories for sensitive images, implementing access controls and authentication for pushing images, and incorporating manual approval steps in the CI/CD pipeline for critical deployments. Additionally, regularly updating base images and dependencies can help mitigate security risks.


